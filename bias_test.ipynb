{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, w, l):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = w\n",
    "        self.l = l\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for item in self.w:\n",
    "            s1, s2, d, b = item\n",
    "            fc = nn.Linear(s1, s2)\n",
    "            fc.weight.data = d\n",
    "            #fc.bias.data = b\n",
    "            self.fc_layers.append(fc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.print_w()\n",
    "        for i in range(len(self.fc_layers)-2):\n",
    "            x = F.relu(self.fc_layers[i](x))\n",
    "        a = F.softmax(F.relu(self.fc_layers[-2](x)), dim=-1)\n",
    "        v = F.relu(self.fc_layers[-1](x))\n",
    "        return a, v\n",
    "\n",
    "    def print_w(self):\n",
    "        for item in self.w:\n",
    "            s1, s2, d, b = item\n",
    "            print(s1, s2, np.shape(d), np.shape(b))\n",
    "        print()\n",
    "        for i, l in enumerate(self.fc_layers):\n",
    "            d = l.weight.data.detach().numpy()\n",
    "            print(\"fc\"+str(i)+\" weights: \", d.shape)\n",
    "            b = l.bias.data.detach().numpy()\n",
    "            print(\"fc\"+str(i)+\" biases: \", b.shape)\n",
    "        print()\n",
    "        \n",
    "    def get_w(self):\n",
    "        w = []\n",
    "        for fc in self.fc_layers:\n",
    "            d = fc.weight.data.detach().numpy()\n",
    "            d = list(np.ravel(d))\n",
    "            w.extend(d)\n",
    "            b = fc.bias.data.detach().numpy()\n",
    "            b = list(np.ravel(b))\n",
    "            w.extend(b)\n",
    "        return w\n",
    "\n",
    "    def set_w(self, w):\n",
    "        self.w = w\n",
    "        for i, item in enumerate(self.w):\n",
    "            s1, s2, d, b = item\n",
    "            self.fc_layers[i].weight.data = d\n",
    "            self.fc_layers[i].bias.data = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5281\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [32,64,32]\n",
    "action_size = 10\n",
    "state_size = 20\n",
    "genome_size = 0\n",
    "genome_size += state_size*hidden_size[0]\n",
    "genome_size += hidden_size[0]\n",
    "if len(hidden_size) > 1:\n",
    "    for i in range(len(hidden_size)):\n",
    "        if i+1 < len(hidden_size):\n",
    "            genome_size += hidden_size[i]*hidden_size[i+1]\n",
    "            bl = max(hidden_size[i], hidden_size[i+1])\n",
    "            genome_size += bl\n",
    "genome_size += action_size*hidden_size[-1]\n",
    "genome_size += hidden_size[-1]\n",
    "genome_size += hidden_size[-1]\n",
    "genome_size += 1\n",
    "print(genome_size)\n",
    "genome = np.random.uniform(-1, 1, genome_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "m1 = 0\n",
    "m2 = state_size * hidden_size[0]\n",
    "m3 = m2 + hidden_size[0]\n",
    "w = torch.Tensor(np.reshape(genome[m1:m2], (hidden_size[0], state_size)))\n",
    "b = torch.Tensor(np.reshape(genome[m2:m3], (hidden_size[0])))\n",
    "weights.append([state_size, hidden_size[0], w, b])\n",
    "if len(hidden_size) > 1:\n",
    "    for i in range(len(hidden_size)):\n",
    "        if i+1 < len(hidden_size):\n",
    "            m1 = m3\n",
    "            m2 = m1 + (hidden_size[i] * hidden_size[i+1])\n",
    "            m3 = m2 + hidden_size[i]\n",
    "            w = torch.Tensor(np.reshape(genome[m1:m2],\n",
    "                             (hidden_size[i+1], hidden_size[i])))\n",
    "            b = torch.Tensor(np.reshape(genome[m2:m3], (hidden_size[i])))\n",
    "            weights.append([hidden_size[i], hidden_size[i+1], w, b])\n",
    "m1 = m3\n",
    "m2 = m1 + action_size*hidden_size[-1]\n",
    "m3 = m2 + action_size\n",
    "w = torch.Tensor(np.reshape(genome[m1:m2], (action_size, hidden_size[-1])))\n",
    "b = torch.Tensor(np.reshape(genome[m2:m3], (action_size)))\n",
    "weights.append([hidden_size[-1], action_size, w, b])\n",
    "m1 = m3\n",
    "m2 = m1 + hidden_size[-1]\n",
    "m3 = m2 + 1\n",
    "w = torch.Tensor(np.reshape(genome[m1:m2], (1, hidden_size[-1])))\n",
    "b = torch.Tensor(np.reshape(genome[m2:m3], (1)))\n",
    "weights.append([hidden_size[-1], 1, w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4146, 0.1531, 0.7824, 0.6306, 0.8297, 0.6776, 0.1704, 0.5467, 0.4947,\n",
      "         0.2913, 0.0253, 0.3067, 0.5047, 0.1875, 0.6361, 0.1143, 0.6513, 0.4027,\n",
      "         0.0687, 0.8890]])\n",
      "20 32 torch.Size([32, 20]) torch.Size([32])\n",
      "32 64 torch.Size([64, 32]) torch.Size([32])\n",
      "64 32 torch.Size([32, 64]) torch.Size([64])\n",
      "32 10 torch.Size([10, 32]) torch.Size([10])\n",
      "32 1 torch.Size([1, 32]) torch.Size([1])\n",
      "\n",
      "fc0 weights:  (32, 20)\n",
      "fc0 biases:  (32,)\n",
      "fc1 weights:  (64, 32)\n",
      "fc1 biases:  (32,)\n",
      "fc2 weights:  (32, 64)\n",
      "fc2 biases:  (64,)\n",
      "fc3 weights:  (10, 32)\n",
      "fc3 biases:  (10,)\n",
      "fc4 weights:  (1, 32)\n",
      "fc4 biases:  (1,)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (64) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [1, 64].  Tensor sizes: [32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a6ccfce9e160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-3cea0b006a4a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (64) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [1, 64].  Tensor sizes: [32]"
     ]
    }
   ],
   "source": [
    "model = Net(weights, True)\n",
    "state = np.random.rand(state_size)\n",
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "print(state)\n",
    "a, v = model(state)\n",
    "print(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc0 weights:  (32, 40)\n",
      "fc0 biases:  (32,)\n",
      "\n",
      "fc1 weights:  (64, 32)\n",
      "fc1 biases:  (64,)\n",
      "\n",
      "fc2 weights:  (32, 64)\n",
      "fc2 biases:  (32,)\n",
      "\n",
      "fc3 weights:  (8, 32)\n",
      "fc3 biases:  (8,)\n",
      "\n",
      "fc4 weights:  (1, 32)\n",
      "fc4 biases:  (1,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = nn.Linear(40, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 8)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        a = F.softmax(F.relu(self.fc4(x)), dim=-1)\n",
    "        v = F.relu(self.fc5(x))\n",
    "        return a, v\n",
    "\n",
    "    def get_w(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3, self.fc4, self.fc5]\n",
    "        for i, l in enumerate(layers):\n",
    "            d = l.weight.data.detach().numpy()\n",
    "            print(\"fc\"+str(i)+\" weights: \", d.shape)\n",
    "            b = l.bias.data.detach().numpy()\n",
    "            print(\"fc\"+str(i)+\" biases: \", b.shape)\n",
    "            print()\n",
    "\n",
    "model2 = Net2()\n",
    "_ = model2.get_w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block: 0\n",
      "fc0 weights:  (8, 4)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "Block: 1\n",
      "fc0 weights:  (8, 5)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "Block: 2\n",
      "fc0 weights:  (8, 4)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "action weights:  (4, 12)\n",
      "action biases:  (4,)\n",
      "value weights:  (1, 12)\n",
      "value biases:  (1,)\n",
      "\n",
      "tensor([[0.3013, 0.9713, 0.1598, 0.9770, 0.9258, 0.6427, 0.0125, 0.4924, 0.2813,\n",
      "         0.0841, 0.4329, 0.4911, 0.3229]])\n",
      "0\n",
      "tensor([0.3013, 0.9713, 0.1598, 0.9770])\n",
      "4\n",
      "tensor([0.9258, 0.6427, 0.0125, 0.4924, 0.2813])\n",
      "9\n",
      "tensor([0.0841, 0.4329, 0.4911, 0.3229])\n",
      "tensor([0.2360, 0.2114, 0.2374, 0.3153], grad_fn=<SoftmaxBackward>) tensor([0.], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-ebc9762688cd>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rc = torch.ravel(torch.tensor(block_out))\n"
     ]
    }
   ],
   "source": [
    "class Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        self.block = [[4, 8], [5, 8], [4, 8]]\n",
    "        self.num_actions = 4\n",
    "        self.num_blocks = len(self.block)\n",
    "        self.inps = [x[0] for x in self.block]\n",
    "        self.out_cat = self.num_blocks * self.num_actions\n",
    "        self.blocks = {}\n",
    "        for index in range(self.num_blocks):\n",
    "            self.blocks[index] = nn.ModuleList()\n",
    "            fc = nn.Linear(self.block[index][0], self.block[index][1])\n",
    "            self.blocks[index].append(fc)\n",
    "            fc = nn.Linear(self.block[index][-1], self.num_actions)\n",
    "            self.blocks[index].append(fc)\n",
    "        self.action = nn.Linear(self.out_cat, self.num_actions)\n",
    "        self.value = nn.Linear(self.out_cat, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        block_out = torch.empty((self.num_blocks, self.num_actions))\n",
    "        current_index = 0\n",
    "        for index in range(len(self.blocks)):\n",
    "            print(current_index)\n",
    "            i = x[0, current_index:current_index+self.inps[index]]\n",
    "            print(i)\n",
    "            a = F.relu(self.blocks[index][0](i))\n",
    "            a = F.relu(self.blocks[index][1](a))\n",
    "            block_out[index] = a\n",
    "            current_index = current_index+self.inps[index]\n",
    "        rc = torch.ravel(torch.tensor(block_out))\n",
    "        a = F.softmax(F.relu(self.action(rc)), dim=-1)\n",
    "        v = F.relu(self.value(rc))\n",
    "        return a, v\n",
    "\n",
    "    def get_w(self):\n",
    "        for index in range(self.num_blocks):\n",
    "            print(\"Block: \" + str(index))\n",
    "            d = self.blocks[index][0].weight.data.detach().numpy()\n",
    "            print(\"fc0 weights: \", d.shape)\n",
    "            b = self.blocks[index][0].bias.data.detach().numpy()\n",
    "            print(\"fc0 biases: \", b.shape)\n",
    "            d = self.blocks[index][1].weight.detach().numpy()\n",
    "            print(\"fc1 weights: \", d.shape)\n",
    "            b = self.blocks[index][1].bias.data.detach().numpy()\n",
    "            print(\"fc1 biases: \", b.shape)\n",
    "        d = self.action.weight.data.detach().numpy()\n",
    "        print(\"action weights: \", d.shape)\n",
    "        b = self.action.bias.data.detach().numpy()\n",
    "        print(\"action biases: \", b.shape)\n",
    "        d = self.value.weight.data.detach().numpy()\n",
    "        print(\"value weights: \", d.shape)\n",
    "        b = self.value.bias.data.detach().numpy()\n",
    "        print(\"value biases: \", b.shape)\n",
    "        print()\n",
    "\n",
    "model3 = Net3()\n",
    "_ = model3.get_w()\n",
    "state = np.random.rand(13)\n",
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "a, v = model3(state)\n",
    "print(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 72, 64, 160, 64, 16]\n",
      "440\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "genome_size = []\n",
    "net_desc = [[4, 8, 4], [5, 8, 4], [4, 8, 4], [6, 16, 4]]\n",
    "action_size = 4 # final output layer\n",
    "state_size = sum([x[0] for x in net_desc])\n",
    "out_cat = sum([x[-1] for x in net_desc])\n",
    "for item in net_desc:\n",
    "    gs = 0\n",
    "    for i in range(len(item)-1):\n",
    "        gs += item[i] * item[i+1]\n",
    "    genome_size.append(gs)\n",
    "action_head = out_cat*action_size\n",
    "genome_size.append(action_head)\n",
    "net_desc.append([out_cat, action_size])\n",
    "value_head = out_cat*1\n",
    "genome_size.append(value_head)\n",
    "net_desc.append([out_cat, 1])\n",
    "state = []\n",
    "for item in genome_size:\n",
    "    state.append(np.random.uniform(-1, 1, item))\n",
    "state = np.array(state)\n",
    "print(genome_size)\n",
    "print(sum(genome_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry: 0\n",
      "4 8 torch.Size([8, 4])\n",
      "8 4 torch.Size([4, 8])\n",
      "Entry: 1\n",
      "5 8 torch.Size([8, 5])\n",
      "8 4 torch.Size([4, 8])\n",
      "Entry: 2\n",
      "4 8 torch.Size([8, 4])\n",
      "8 4 torch.Size([4, 8])\n",
      "Entry: 3\n",
      "6 16 torch.Size([16, 6])\n",
      "16 4 torch.Size([4, 16])\n",
      "Entry: 4\n",
      "16 4 torch.Size([4, 16])\n",
      "Entry: 5\n",
      "16 1 torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "for index, item in enumerate(state):\n",
    "    entry = []\n",
    "    layer_desc = net_desc[index]\n",
    "    if len(layer_desc) > 2:\n",
    "        s1, s2, o = layer_desc\n",
    "        w = torch.Tensor(np.reshape(item[0:s1*s2], (s2, s1)))\n",
    "        entry.append([s1, s2, w])\n",
    "        w = torch.Tensor(np.reshape(item[s1*s2:], (o, s2)))\n",
    "        entry.append([s2, o, w])\n",
    "    else:\n",
    "        s1, o = layer_desc\n",
    "        w = torch.Tensor(np.reshape(item, (o, s1)))\n",
    "        entry.append([s1, o, w])\n",
    "    weights.append(entry)\n",
    "for index, entry in enumerate(weights):\n",
    "    print(\"Entry:\", index)\n",
    "    for e in entry:\n",
    "        print(e[0], e[1], e[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks [[4, 8, 4], [5, 8, 4], [4, 8, 4], [6, 16, 4]]\n",
      "actions 4\n",
      "num blocks 4\n",
      "inps [4, 5, 4, 6]\n",
      "16\n",
      "Block: 0\n",
      "fc0 weights:  (8, 4)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "Block: 1\n",
      "fc0 weights:  (8, 5)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "Block: 2\n",
      "fc0 weights:  (8, 4)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "Block: 3\n",
      "fc0 weights:  (16, 6)\n",
      "fc0 biases:  (16,)\n",
      "fc1 weights:  (4, 16)\n",
      "fc1 biases:  (4,)\n",
      "action weights:  (4, 16)\n",
      "action biases:  (4,)\n",
      "value weights:  (1, 16)\n",
      "value biases:  (1,)\n",
      "\n",
      "tensor([[0.2772, 0.2074, 0.1915, 0.8742, 0.3553, 0.8071, 0.9260, 0.4212, 0.7631,\n",
      "         0.3541, 0.1544, 0.2500, 0.6699, 0.1226, 0.7051, 0.3138, 0.4757, 0.5638,\n",
      "         0.1946]])\n",
      "tensor([[0.2772, 0.2074, 0.1915, 0.8742, 0.3553, 0.8071, 0.9260, 0.4212, 0.7631,\n",
      "         0.3541, 0.1544, 0.2500, 0.6699, 0.1226, 0.7051, 0.3138, 0.4757, 0.5638,\n",
      "         0.1946]])\n",
      "0\n",
      "tensor([0.2772, 0.2074, 0.1915, 0.8742])\n",
      "4\n",
      "tensor([0.3553, 0.8071, 0.9260, 0.4212, 0.7631])\n",
      "9\n",
      "tensor([0.3541, 0.1544, 0.2500, 0.6699])\n",
      "13\n",
      "tensor([0.1226, 0.7051, 0.3138, 0.4757, 0.5638, 0.1946])\n",
      "tensor([0.3098, 0.2208, 0.2208, 0.2485], grad_fn=<SoftmaxBackward>) tensor([0.], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-129-6ad7651343f6>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rc = torch.ravel(torch.tensor(block_out))\n"
     ]
    }
   ],
   "source": [
    "class Net4(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(Net4, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.block = []\n",
    "        for item in weights:\n",
    "            if len(item) > 1:\n",
    "                self.block.append([item[0][0], item[0][1], item[1][1]])\n",
    "        print(\"blocks\", self.block)\n",
    "        self.num_actions = self.weights[-2][0][1]\n",
    "        print(\"actions\", self.num_actions)\n",
    "        self.num_blocks = len(self.block)\n",
    "        print(\"num blocks\", self.num_blocks)\n",
    "        self.inps = [x[0] for x in self.block]\n",
    "        print(\"inps\", self.inps)\n",
    "        self.out_cat = sum([x[-1] for x in self.block])\n",
    "        print(self.out_cat)\n",
    "        self.blocks = {}\n",
    "        for index in range(self.num_blocks):\n",
    "            weights1 = self.weights[index][0][2]\n",
    "            weights2 = self.weights[index][1][2]\n",
    "            self.blocks[index] = nn.ModuleList()\n",
    "            fc = nn.Linear(self.block[index][0], self.block[index][1])\n",
    "            fc.weight.data = weights1\n",
    "            self.blocks[index].append(fc)\n",
    "            fc = nn.Linear(self.block[index][1], self.block[index][2])\n",
    "            fc.weight.data = weights2\n",
    "            self.blocks[index].append(fc)\n",
    "        self.action = nn.Linear(self.out_cat, self.num_actions)\n",
    "        self.value = nn.Linear(self.out_cat, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        block_out = torch.empty((self.num_blocks, self.num_actions))\n",
    "        current_index = 0\n",
    "        for index in range(len(self.blocks)):\n",
    "            print(current_index)\n",
    "            i = x[0, current_index:current_index+self.inps[index]]\n",
    "            print(i)\n",
    "            a = F.relu(self.blocks[index][0](i))\n",
    "            a = F.relu(self.blocks[index][1](a))\n",
    "            block_out[index] = a\n",
    "            current_index = current_index+self.inps[index]\n",
    "        rc = torch.ravel(torch.tensor(block_out))\n",
    "        a = F.softmax(F.relu(self.action(rc)), dim=-1)\n",
    "        v = F.relu(self.value(rc))\n",
    "        return a, v\n",
    "\n",
    "    def get_w(self):\n",
    "        for index in range(self.num_blocks):\n",
    "            print(\"Block: \" + str(index))\n",
    "            d = self.blocks[index][0].weight.data.detach().numpy()\n",
    "            print(\"fc0 weights: \", d.shape)\n",
    "            b = self.blocks[index][0].bias.data.detach().numpy()\n",
    "            print(\"fc0 biases: \", b.shape)\n",
    "            d = self.blocks[index][1].weight.detach().numpy()\n",
    "            print(\"fc1 weights: \", d.shape)\n",
    "            b = self.blocks[index][1].bias.data.detach().numpy()\n",
    "            print(\"fc1 biases: \", b.shape)\n",
    "        d = self.action.weight.data.detach().numpy()\n",
    "        print(\"action weights: \", d.shape)\n",
    "        b = self.action.bias.data.detach().numpy()\n",
    "        print(\"action biases: \", b.shape)\n",
    "        d = self.value.weight.data.detach().numpy()\n",
    "        print(\"value weights: \", d.shape)\n",
    "        b = self.value.bias.data.detach().numpy()\n",
    "        print(\"value biases: \", b.shape)\n",
    "        print()\n",
    "\n",
    "model4 = Net4(weights)\n",
    "_ = model4.get_w()\n",
    "state = np.random.rand(4 + 5 + 4 + 6)\n",
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "print(state)\n",
    "a, v = model4(state)\n",
    "print(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
