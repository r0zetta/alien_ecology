{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, w, l):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = w\n",
    "        self.l = l\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for item in self.w:\n",
    "            s1, s2, d, b = item\n",
    "            fc = nn.Linear(s1, s2)\n",
    "            fc.weight.data = d\n",
    "            #fc.bias.data = b\n",
    "            self.fc_layers.append(fc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.print_w()\n",
    "        for i in range(len(self.fc_layers)-2):\n",
    "            x = F.relu(self.fc_layers[i](x))\n",
    "        a = F.softmax(F.relu(self.fc_layers[-2](x)), dim=-1)\n",
    "        v = F.relu(self.fc_layers[-1](x))\n",
    "        return a, v\n",
    "\n",
    "    def print_w(self):\n",
    "        for item in self.w:\n",
    "            s1, s2, d, b = item\n",
    "            print(s1, s2, np.shape(d), np.shape(b))\n",
    "        print()\n",
    "        for i, l in enumerate(self.fc_layers):\n",
    "            d = l.weight.data.detach().numpy()\n",
    "            print(\"fc\"+str(i)+\" weights: \", d.shape)\n",
    "            b = l.bias.data.detach().numpy()\n",
    "            print(\"fc\"+str(i)+\" biases: \", b.shape)\n",
    "        print()\n",
    "        \n",
    "    def get_w(self):\n",
    "        w = []\n",
    "        for fc in self.fc_layers:\n",
    "            d = fc.weight.data.detach().numpy()\n",
    "            d = list(np.ravel(d))\n",
    "            w.extend(d)\n",
    "            b = fc.bias.data.detach().numpy()\n",
    "            b = list(np.ravel(b))\n",
    "            w.extend(b)\n",
    "        return w\n",
    "\n",
    "    def set_w(self, w):\n",
    "        self.w = w\n",
    "        for i, item in enumerate(self.w):\n",
    "            s1, s2, d, b = item\n",
    "            self.fc_layers[i].weight.data = d\n",
    "            self.fc_layers[i].bias.data = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5281\n"
     ]
    }
   ],
   "source": [
    "hidden_size = [32,64,32]\n",
    "action_size = 10\n",
    "state_size = 20\n",
    "genome_size = 0\n",
    "genome_size += state_size*hidden_size[0]\n",
    "genome_size += hidden_size[0]\n",
    "if len(hidden_size) > 1:\n",
    "    for i in range(len(hidden_size)):\n",
    "        if i+1 < len(hidden_size):\n",
    "            genome_size += hidden_size[i]*hidden_size[i+1]\n",
    "            bl = max(hidden_size[i], hidden_size[i+1])\n",
    "            genome_size += bl\n",
    "genome_size += action_size*hidden_size[-1]\n",
    "genome_size += hidden_size[-1]\n",
    "genome_size += hidden_size[-1]\n",
    "genome_size += 1\n",
    "print(genome_size)\n",
    "genome = np.random.uniform(-1, 1, genome_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "m1 = 0\n",
    "m2 = state_size * hidden_size[0]\n",
    "m3 = m2 + hidden_size[0]\n",
    "w = torch.Tensor(np.reshape(genome[m1:m2], (hidden_size[0], state_size)))\n",
    "b = torch.Tensor(np.reshape(genome[m2:m3], (hidden_size[0])))\n",
    "weights.append([state_size, hidden_size[0], w, b])\n",
    "if len(hidden_size) > 1:\n",
    "    for i in range(len(hidden_size)):\n",
    "        if i+1 < len(hidden_size):\n",
    "            m1 = m3\n",
    "            m2 = m1 + (hidden_size[i] * hidden_size[i+1])\n",
    "            m3 = m2 + hidden_size[i]\n",
    "            w = torch.Tensor(np.reshape(genome[m1:m2],\n",
    "                             (hidden_size[i+1], hidden_size[i])))\n",
    "            b = torch.Tensor(np.reshape(genome[m2:m3], (hidden_size[i])))\n",
    "            weights.append([hidden_size[i], hidden_size[i+1], w, b])\n",
    "m1 = m3\n",
    "m2 = m1 + action_size*hidden_size[-1]\n",
    "m3 = m2 + action_size\n",
    "w = torch.Tensor(np.reshape(genome[m1:m2], (action_size, hidden_size[-1])))\n",
    "b = torch.Tensor(np.reshape(genome[m2:m3], (action_size)))\n",
    "weights.append([hidden_size[-1], action_size, w, b])\n",
    "m1 = m3\n",
    "m2 = m1 + hidden_size[-1]\n",
    "m3 = m2 + 1\n",
    "w = torch.Tensor(np.reshape(genome[m1:m2], (1, hidden_size[-1])))\n",
    "b = torch.Tensor(np.reshape(genome[m2:m3], (1)))\n",
    "weights.append([hidden_size[-1], 1, w, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7167, 0.7243, 0.2666, 0.0309, 0.1190, 0.9754, 0.0312, 0.9944, 0.9415,\n",
      "         0.5615, 0.3444, 0.3882, 0.2003, 0.1937, 0.6117, 0.4140, 0.0930, 0.9652,\n",
      "         0.6613, 0.7046]])\n",
      "20 32 torch.Size([32, 20]) torch.Size([32])\n",
      "32 64 torch.Size([64, 32]) torch.Size([32])\n",
      "64 32 torch.Size([32, 64]) torch.Size([64])\n",
      "32 10 torch.Size([10, 32]) torch.Size([10])\n",
      "32 1 torch.Size([1, 32]) torch.Size([1])\n",
      "\n",
      "fc0 weights:  (32, 20)\n",
      "fc0 biases:  (32,)\n",
      "fc1 weights:  (64, 32)\n",
      "fc1 biases:  (64,)\n",
      "fc2 weights:  (32, 64)\n",
      "fc2 biases:  (32,)\n",
      "fc3 weights:  (10, 32)\n",
      "fc3 biases:  (10,)\n",
      "fc4 weights:  (1, 32)\n",
      "fc4 biases:  (1,)\n",
      "\n",
      "tensor([[4.9324e-16, 4.9324e-16, 2.5017e-12, 4.9324e-16, 6.8106e-15, 1.0000e+00,\n",
      "         4.9324e-16, 4.9324e-16, 4.9324e-16, 1.5233e-09]],\n",
      "       grad_fn=<SoftmaxBackward>) tensor([[49.1205]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Net(weights, True)\n",
    "state = np.random.rand(state_size)\n",
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "print(state)\n",
    "a, v = model(state)\n",
    "print(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc0 weights:  (32, 40)\n",
      "fc0 biases:  (32,)\n",
      "\n",
      "fc1 weights:  (64, 32)\n",
      "fc1 biases:  (64,)\n",
      "\n",
      "fc2 weights:  (32, 64)\n",
      "fc2 biases:  (32,)\n",
      "\n",
      "fc3 weights:  (8, 32)\n",
      "fc3 biases:  (8,)\n",
      "\n",
      "fc4 weights:  (1, 32)\n",
      "fc4 biases:  (1,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = nn.Linear(40, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 8)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        a = F.softmax(F.relu(self.fc4(x)), dim=-1)\n",
    "        v = F.relu(self.fc5(x))\n",
    "        return a, v\n",
    "\n",
    "    def get_w(self):\n",
    "        layers = [self.fc1, self.fc2, self.fc3, self.fc4, self.fc5]\n",
    "        for i, l in enumerate(layers):\n",
    "            d = l.weight.data.detach().numpy()\n",
    "            print(\"fc\"+str(i)+\" weights: \", d.shape)\n",
    "            b = l.bias.data.detach().numpy()\n",
    "            print(\"fc\"+str(i)+\" biases: \", b.shape)\n",
    "            print()\n",
    "\n",
    "model2 = Net2()\n",
    "_ = model2.get_w()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block: 0\n",
      "fc0 weights:  (8, 4)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "Block: 1\n",
      "fc0 weights:  (8, 5)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "Block: 2\n",
      "fc0 weights:  (8, 4)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "action weights:  (4, 12)\n",
      "action biases:  (4,)\n",
      "value weights:  (1, 12)\n",
      "value biases:  (1,)\n",
      "\n",
      "tensor([[0.6031, 0.8389, 0.0230, 0.6333, 0.7610, 0.4338, 0.7071, 0.4419, 0.8627,\n",
      "         0.9455, 0.7478, 0.7761, 0.9690]])\n",
      "0\n",
      "tensor([0.6031, 0.8389, 0.0230, 0.6333])\n",
      "4\n",
      "tensor([0.7610, 0.4338, 0.7071, 0.4419, 0.8627])\n",
      "9\n",
      "tensor([0.9455, 0.7478, 0.7761, 0.9690])\n",
      "tensor([0.2823, 0.2659, 0.2259, 0.2259], grad_fn=<SoftmaxBackward>) tensor([0.], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-ebc9762688cd>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rc = torch.ravel(torch.tensor(block_out))\n"
     ]
    }
   ],
   "source": [
    "class Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        self.block = [[4, 8], [5, 8], [4, 8]]\n",
    "        self.num_actions = 4\n",
    "        self.num_blocks = len(self.block)\n",
    "        self.inps = [x[0] for x in self.block]\n",
    "        self.out_cat = self.num_blocks * self.num_actions\n",
    "        self.blocks = {}\n",
    "        for index in range(self.num_blocks):\n",
    "            self.blocks[index] = nn.ModuleList()\n",
    "            fc = nn.Linear(self.block[index][0], self.block[index][1])\n",
    "            self.blocks[index].append(fc)\n",
    "            fc = nn.Linear(self.block[index][-1], self.num_actions)\n",
    "            self.blocks[index].append(fc)\n",
    "        self.action = nn.Linear(self.out_cat, self.num_actions)\n",
    "        self.value = nn.Linear(self.out_cat, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        block_out = torch.empty((self.num_blocks, self.num_actions))\n",
    "        current_index = 0\n",
    "        for index in range(len(self.blocks)):\n",
    "            print(current_index)\n",
    "            i = x[0, current_index:current_index+self.inps[index]]\n",
    "            print(i)\n",
    "            a = F.relu(self.blocks[index][0](i))\n",
    "            a = F.relu(self.blocks[index][1](a))\n",
    "            block_out[index] = a\n",
    "            current_index = current_index+self.inps[index]\n",
    "        rc = torch.ravel(torch.tensor(block_out))\n",
    "        a = F.softmax(F.relu(self.action(rc)), dim=-1)\n",
    "        v = F.relu(self.value(rc))\n",
    "        return a, v\n",
    "\n",
    "    def get_w(self):\n",
    "        for index in range(self.num_blocks):\n",
    "            print(\"Block: \" + str(index))\n",
    "            d = self.blocks[index][0].weight.data.detach().numpy()\n",
    "            print(\"fc0 weights: \", d.shape)\n",
    "            b = self.blocks[index][0].bias.data.detach().numpy()\n",
    "            print(\"fc0 biases: \", b.shape)\n",
    "            d = self.blocks[index][1].weight.detach().numpy()\n",
    "            print(\"fc1 weights: \", d.shape)\n",
    "            b = self.blocks[index][1].bias.data.detach().numpy()\n",
    "            print(\"fc1 biases: \", b.shape)\n",
    "        d = self.action.weight.data.detach().numpy()\n",
    "        print(\"action weights: \", d.shape)\n",
    "        b = self.action.bias.data.detach().numpy()\n",
    "        print(\"action biases: \", b.shape)\n",
    "        d = self.value.weight.data.detach().numpy()\n",
    "        print(\"value weights: \", d.shape)\n",
    "        b = self.value.bias.data.detach().numpy()\n",
    "        print(\"value biases: \", b.shape)\n",
    "        print()\n",
    "\n",
    "model3 = Net3()\n",
    "_ = model3.get_w()\n",
    "state = np.random.rand(13)\n",
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "a, v = model3(state)\n",
    "print(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 90, 64, 48, 12]\n",
      "278\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "action_size = 4 # final output layer\n",
    "genome_size = []\n",
    "net_desc = [[4, 8], [5, 10], [4, 8]]\n",
    "for index in range(len(net_desc)):\n",
    "    net_desc[index].append(action_size)\n",
    "state_size = sum([x[0] for x in net_desc])\n",
    "out_cat = sum([x[-1] for x in net_desc])\n",
    "for item in net_desc:\n",
    "    gs = 0\n",
    "    for i in range(len(item)-1):\n",
    "        gs += item[i] * item[i+1]\n",
    "    genome_size.append(gs)\n",
    "action_head = out_cat*action_size\n",
    "genome_size.append(action_head)\n",
    "net_desc.append([out_cat, action_size])\n",
    "value_head = out_cat*1\n",
    "genome_size.append(value_head)\n",
    "net_desc.append([out_cat, 1])\n",
    "state = []\n",
    "for item in genome_size:\n",
    "    state.append(np.random.randint(-1, 2, item))\n",
    "state = np.array(state)\n",
    "print(genome_size)\n",
    "print(sum(genome_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry: 0\n",
      "4 8 torch.Size([8, 4])\n",
      "8 4 torch.Size([4, 8])\n",
      "Entry: 1\n",
      "5 10 torch.Size([10, 5])\n",
      "10 4 torch.Size([4, 10])\n",
      "Entry: 2\n",
      "4 8 torch.Size([8, 4])\n",
      "8 4 torch.Size([4, 8])\n",
      "Entry: 3\n",
      "12 4 torch.Size([4, 12])\n",
      "Entry: 4\n",
      "12 1 torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "for index, item in enumerate(state):\n",
    "    entry = []\n",
    "    layer_desc = net_desc[index]\n",
    "    if len(layer_desc) > 2:\n",
    "        s1, s2, o = layer_desc\n",
    "        w = torch.Tensor(np.reshape(item[0:s1*s2], (s2, s1)))\n",
    "        entry.append([s1, s2, w])\n",
    "        w = torch.Tensor(np.reshape(item[s1*s2:], (o, s2)))\n",
    "        entry.append([s2, o, w])\n",
    "    else:\n",
    "        s1, o = layer_desc\n",
    "        w = torch.Tensor(np.reshape(item, (o, s1)))\n",
    "        entry.append([s1, o, w])\n",
    "    weights.append(entry)\n",
    "for index, entry in enumerate(weights):\n",
    "    print(\"Entry:\", index)\n",
    "    for e in entry:\n",
    "        print(e[0], e[1], e[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks [[4, 8, 4], [5, 10, 4], [4, 8, 4]]\n",
      "actions 4\n",
      "num blocks 3\n",
      "inps [4, 5, 4]\n",
      "12\n",
      "Block: 0\n",
      "fc0 weights:  (8, 4)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "Block: 1\n",
      "fc0 weights:  (10, 5)\n",
      "fc0 biases:  (10,)\n",
      "fc1 weights:  (4, 10)\n",
      "fc1 biases:  (4,)\n",
      "Block: 2\n",
      "fc0 weights:  (8, 4)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "action weights:  (4, 12)\n",
      "torch.Size([4, 12])\n",
      "action biases:  (4,)\n",
      "value weights:  (1, 12)\n",
      "torch.Size([1, 12])\n",
      "value biases:  (1,)\n",
      "total params:  278\n",
      "[64, 90, 64, 48, 12]\n",
      "\n",
      "tensor([[0.8946, 0.9464, 0.2177, 0.1877, 0.0674, 0.0933, 0.6077, 0.5498, 0.6394,\n",
      "         0.6518, 0.9558, 0.5961, 0.0387, 0.0898, 0.9786, 0.3518, 0.5415, 0.0609,\n",
      "         0.0914]])\n",
      "tensor([[0.8946, 0.9464, 0.2177, 0.1877, 0.0674, 0.0933, 0.6077, 0.5498, 0.6394,\n",
      "         0.6518, 0.9558, 0.5961, 0.0387, 0.0898, 0.9786, 0.3518, 0.5415, 0.0609,\n",
      "         0.0914]])\n",
      "0\n",
      "tensor([0.8946, 0.9464, 0.2177, 0.1877])\n",
      "4\n",
      "tensor([0.0674, 0.0933, 0.6077, 0.5498, 0.6394])\n",
      "9\n",
      "tensor([0.6518, 0.9558, 0.5961, 0.0387])\n",
      "tensor([5.3744e-06, 1.2787e-06, 1.2787e-06, 9.9999e-01],\n",
      "       grad_fn=<SoftmaxBackward>) tensor([0.], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-65b4104716ac>:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rc = torch.ravel(torch.tensor(block_out))\n"
     ]
    }
   ],
   "source": [
    "class Net4(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(Net4, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.block = []\n",
    "        for item in weights:\n",
    "            if len(item) > 1:\n",
    "                self.block.append([item[0][0], item[0][1], item[1][1]])\n",
    "        print(\"blocks\", self.block)\n",
    "        self.num_actions = self.weights[-2][0][1]\n",
    "        print(\"actions\", self.num_actions)\n",
    "        self.num_blocks = len(self.block)\n",
    "        print(\"num blocks\", self.num_blocks)\n",
    "        self.inps = [x[0] for x in self.block]\n",
    "        print(\"inps\", self.inps)\n",
    "        self.out_cat = sum([x[-1] for x in self.block])\n",
    "        print(self.out_cat)\n",
    "        self.blocks = {}\n",
    "        for index in range(self.num_blocks):\n",
    "            weights1 = self.weights[index][0][2]\n",
    "            weights2 = self.weights[index][1][2]\n",
    "            self.blocks[index] = nn.ModuleList()\n",
    "            fc = nn.Linear(self.block[index][0], self.block[index][1])\n",
    "            fc.weight.data = weights1\n",
    "            self.blocks[index].append(fc)\n",
    "            fc = nn.Linear(self.block[index][1], self.block[index][2])\n",
    "            fc.weight.data = weights2\n",
    "            self.blocks[index].append(fc)\n",
    "        self.action = nn.Linear(self.out_cat, self.num_actions)\n",
    "        self.action.weight.data = self.weights[-2][0][2]\n",
    "        self.value = nn.Linear(self.out_cat, 1)\n",
    "        self.value.weight.data = self.weights[-1][0][2]\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        block_out = torch.empty((self.num_blocks, self.num_actions))\n",
    "        current_index = 0\n",
    "        for index in range(len(self.blocks)):\n",
    "            print(current_index)\n",
    "            i = x[0, current_index:current_index+self.inps[index]]\n",
    "            print(i)\n",
    "            a = F.relu(self.blocks[index][0](i))\n",
    "            a = F.relu(self.blocks[index][1](a))\n",
    "            block_out[index] = a\n",
    "            current_index = current_index+self.inps[index]\n",
    "        rc = torch.ravel(torch.tensor(block_out))\n",
    "        a = F.softmax(F.relu(self.action(rc)), dim=-1)\n",
    "        v = F.relu(self.value(rc))\n",
    "        return a, v\n",
    "\n",
    "    def get_param_count(self, item):\n",
    "        count = 1\n",
    "        for c in item.shape:\n",
    "            count = count * c\n",
    "        return count\n",
    "    \n",
    "    def get_w(self):\n",
    "        total_params = 0\n",
    "        genome = []\n",
    "        for index in range(self.num_blocks):\n",
    "            entry = []\n",
    "            print(\"Block: \" + str(index))\n",
    "            d1 = self.blocks[index][0].weight.data.detach().numpy()\n",
    "            print(\"fc0 weights: \", d1.shape)\n",
    "            total_params += self.get_param_count(d1)\n",
    "            d1 = np.ravel(d1)\n",
    "            entry.extend(list(d1))\n",
    "            b1 = self.blocks[index][0].bias.data.detach().numpy()\n",
    "            print(\"fc0 biases: \", b1.shape)\n",
    "            d2 = self.blocks[index][1].weight.detach().numpy()\n",
    "            print(\"fc1 weights: \", d2.shape)\n",
    "            d2 = np.ravel(d2)\n",
    "            entry.extend(list(d2))\n",
    "            total_params += self.get_param_count(d2)\n",
    "            b2 = self.blocks[index][1].bias.data.detach().numpy()\n",
    "            print(\"fc1 biases: \", b2.shape)\n",
    "            entry = np.ravel(entry)\n",
    "            genome.append(entry)\n",
    "        da = self.action.weight.data.detach().numpy()\n",
    "        genome.append(np.ravel(da))\n",
    "        total_params += self.get_param_count(da)\n",
    "        print(\"action weights: \", da.shape)\n",
    "        print(self.weights[-2][0][2].shape)\n",
    "        ba = self.action.bias.data.detach().numpy()\n",
    "        print(\"action biases: \", ba.shape)\n",
    "        dv = self.value.weight.data.detach().numpy()\n",
    "        genome.append(np.ravel(dv))\n",
    "        total_params += self.get_param_count(dv)\n",
    "        print(\"value weights: \", dv.shape)\n",
    "        print(self.weights[-1][0][2].shape)\n",
    "        bv = self.value.bias.data.detach().numpy()\n",
    "        print(\"value biases: \", bv.shape)\n",
    "        print(\"total params: \", total_params)\n",
    "        genome_shape = [len(x) for x in genome]\n",
    "        print(genome_shape)\n",
    "        print()\n",
    "\n",
    "model4 = Net4(weights)\n",
    "_ = model4.get_w()\n",
    "state = np.random.rand(4 + 5 + 4 + 6)\n",
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "print(state)\n",
    "a, v = model4(state)\n",
    "print(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks [[4, 8, 4], [5, 10, 4], [4, 8, 4]]\n",
      "actions 4\n",
      "num blocks 3\n",
      "inps [4, 5, 4]\n",
      "12\n",
      "Block: 0\n",
      "fc0 weights:  (8, 4)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "Block: 1\n",
      "fc0 weights:  (10, 5)\n",
      "fc0 biases:  (10,)\n",
      "fc1 weights:  (4, 10)\n",
      "fc1 biases:  (4,)\n",
      "Block: 2\n",
      "fc0 weights:  (8, 4)\n",
      "fc0 biases:  (8,)\n",
      "fc1 weights:  (4, 8)\n",
      "fc1 biases:  (4,)\n",
      "action weights:  (4, 12)\n",
      "torch.Size([4, 12])\n",
      "action biases:  (4,)\n",
      "value weights:  (1, 12)\n",
      "torch.Size([1, 12])\n",
      "value biases:  (1,)\n",
      "total params:  278\n",
      "[64, 90, 64, 48, 12]\n",
      "\n",
      "tensor([[0.3943, 0.8254, 0.4910, 0.6994, 0.5332, 0.7207, 0.7378, 0.2389, 0.9107,\n",
      "         0.0131, 0.2315, 0.7641, 0.8478, 0.4742, 0.6365, 0.8244, 0.3221, 0.6580,\n",
      "         0.4473]])\n",
      "tensor([[0.3943, 0.8254, 0.4910, 0.6994, 0.5332, 0.7207, 0.7378, 0.2389, 0.9107,\n",
      "         0.0131, 0.2315, 0.7641, 0.8478, 0.4742, 0.6365, 0.8244, 0.3221, 0.6580,\n",
      "         0.4473]])\n",
      "0\n",
      "tensor([0.3943, 0.8254, 0.4910, 0.6994])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cb7e8c4e1959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-cb7e8c4e1959>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mblock_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mcurrent_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "class Net5(nn.Module):\n",
    "    def __init__(self, weights):\n",
    "        super(Net5, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.block = []\n",
    "        for item in weights:\n",
    "            if len(item) > 1:\n",
    "                self.block.append([item[0][0], item[0][1], item[1][1]])\n",
    "        print(\"blocks\", self.block)\n",
    "        self.num_actions = self.weights[-2][0][1]\n",
    "        print(\"actions\", self.num_actions)\n",
    "        self.num_blocks = len(self.block)\n",
    "        print(\"num blocks\", self.num_blocks)\n",
    "        self.inps = [x[0] for x in self.block]\n",
    "        print(\"inps\", self.inps)\n",
    "        self.out_cat = sum([x[-1] for x in self.block])\n",
    "        print(self.out_cat)\n",
    "        self.blocks = {}\n",
    "        for index in range(self.num_blocks):\n",
    "            weights1 = self.weights[index][0][2]\n",
    "            weights2 = self.weights[index][1][2]\n",
    "            self.blocks[index] = nn.ModuleList()\n",
    "            fc = nn.Linear(self.block[index][0], self.block[index][1])\n",
    "            fc.weight.data = weights1\n",
    "            self.blocks[index].append(fc)\n",
    "            fc = nn.Linear(self.block[index][1], self.block[index][2])\n",
    "            fc.weight.data = weights2\n",
    "            self.blocks[index].append(fc)\n",
    "        self.action = nn.Linear(self.out_cat, self.num_actions)\n",
    "        self.action.weight.data = self.weights[-2][0][2]\n",
    "        self.value = nn.Linear(self.out_cat, 1)\n",
    "        self.value.weight.data = self.weights[-1][0][2]\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        block_out = torch.empty((self.num_blocks, self.num_actions))\n",
    "        current_index = 0\n",
    "        for index in range(len(self.blocks)):\n",
    "            print(current_index)\n",
    "            i = x[0, current_index:current_index+self.inps[index]]\n",
    "            print(i)\n",
    "            a = F.relu(self.blocks[index][0](i))\n",
    "            a = F.relu(self.blocks[index][1](a))\n",
    "            z = torch.zeros_like(a)\n",
    "            z[torch.argmax(a)] = 1.0\n",
    "            block_out[index] = z\n",
    "            current_index = current_index+self.inps[index]\n",
    "        rc = torch.ravel(torch.tensor(block_out))\n",
    "        a = F.softmax(F.relu(self.action(rc)), dim=-1)\n",
    "        v = F.relu(self.value(rc))\n",
    "        return a, v\n",
    "\n",
    "    def get_param_count(self, item):\n",
    "        count = 1\n",
    "        for c in item.shape:\n",
    "            count = count * c\n",
    "        return count\n",
    "    \n",
    "    def get_w(self):\n",
    "        total_params = 0\n",
    "        genome = []\n",
    "        for index in range(self.num_blocks):\n",
    "            entry = []\n",
    "            print(\"Block: \" + str(index))\n",
    "            d1 = self.blocks[index][0].weight.data.detach().numpy()\n",
    "            print(\"fc0 weights: \", d1.shape)\n",
    "            total_params += self.get_param_count(d1)\n",
    "            d1 = np.ravel(d1)\n",
    "            entry.extend(list(d1))\n",
    "            b1 = self.blocks[index][0].bias.data.detach().numpy()\n",
    "            print(\"fc0 biases: \", b1.shape)\n",
    "            d2 = self.blocks[index][1].weight.detach().numpy()\n",
    "            print(\"fc1 weights: \", d2.shape)\n",
    "            d2 = np.ravel(d2)\n",
    "            entry.extend(list(d2))\n",
    "            total_params += self.get_param_count(d2)\n",
    "            b2 = self.blocks[index][1].bias.data.detach().numpy()\n",
    "            print(\"fc1 biases: \", b2.shape)\n",
    "            entry = np.ravel(entry)\n",
    "            genome.append(entry)\n",
    "        da = self.action.weight.data.detach().numpy()\n",
    "        genome.append(np.ravel(da))\n",
    "        total_params += self.get_param_count(da)\n",
    "        print(\"action weights: \", da.shape)\n",
    "        print(self.weights[-2][0][2].shape)\n",
    "        ba = self.action.bias.data.detach().numpy()\n",
    "        print(\"action biases: \", ba.shape)\n",
    "        dv = self.value.weight.data.detach().numpy()\n",
    "        genome.append(np.ravel(dv))\n",
    "        total_params += self.get_param_count(dv)\n",
    "        print(\"value weights: \", dv.shape)\n",
    "        print(self.weights[-1][0][2].shape)\n",
    "        bv = self.value.bias.data.detach().numpy()\n",
    "        print(\"value biases: \", bv.shape)\n",
    "        print(\"total params: \", total_params)\n",
    "        genome_shape = [len(x) for x in genome]\n",
    "        print(genome_shape)\n",
    "        print()\n",
    "\n",
    "model5 = Net5(weights)\n",
    "_ = model5.get_w()\n",
    "state = np.random.rand(4 + 5 + 4 + 6)\n",
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "print(state)\n",
    "a, v = model5(state)\n",
    "print(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
